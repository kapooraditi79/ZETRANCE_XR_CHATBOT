{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## INSTALLING THE DEPENDENCIES\n",
        "\n"
      ],
      "metadata": {
        "id": "6QBxiSQqft2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip\n",
        "!pip install llama-index transformers\n",
        "!pip install langchain openai\n",
        "!pip install transformers flask\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "PI_quGyGEmzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.            #Install all the packages from requiremts.txt\n",
        "!pip install -U -r requirements.txt      #Install any upgrades in the txt file\n",
        "# !pip install flask                     #Couldnt implement this on code"
      ],
      "metadata": {
        "id": "A-U3mqR8Gz0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                                                #LOADING THE DATA\n",
        "\n",
        "from langchain_community.document_loaders import UnstructuredURLLoader\n",
        "urls = ['https://zetrance.com/',\n",
        "        'https://zetrance.com/studio.php',\n",
        "        'https://zetrance.com/arvr.php',\n",
        "        'https://zetrance.com/zenna.php',\n",
        "        'https://zetrance.com/product-visualization.php',\n",
        "        'https://zetrance.com/simulator.php',\n",
        "        'https://zetrance.com/evvr.php',\n",
        "        'https://zetrance.com/btvr.php',\n",
        "        'https://zetrance.com/medvr.php',\n",
        "        'https://zetrance.com/dtvr.php',\n",
        "        'https://zetrance.com/itvr.php',\n",
        "        'https://zetrance.com/coe.php',\n",
        "        'https://zetrance.com/contact.php',\n",
        "        ]\n",
        "\n",
        "#STORING ALL THE PAGES OF ZETRANCE XR WEBSITE IN UNSTRUCTURED URL LOADER\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "data = loader.load()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8KzXItdVKhSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "StRMxHUvOzGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTING STREAMLIT, IT WILL ALLOW US A GUI for the chatbot\n",
        "\n",
        "\n",
        "import streamlit as st\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
        "docs = text_splitter.split_documents(data)\n",
        "\n",
        "all_splits=docs\n",
        "print(len(docs))\n"
      ],
      "metadata": {
        "id": "t5eKRUXSh94s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "id": "Q2U_dEWoinRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SETTING THE EMBEDDINGS"
      ],
      "metadata": {
        "id": "itkeapXvjAoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SETTING UP THE EMBEDDINGS\n",
        "#Importing the LLM model as per your choice\n",
        "#Importing Embeddings to store it into the database\n",
        "#The .env file must be downloaded, it should have openai-api-key\n",
        "#Another way is that we can directly embed our OPENAI-API KEY into the colab nb\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(override=True)\n",
        "\n",
        "st.title(\"ZETRANCE XR CHATBOT\")\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
        "#store into vector database"
      ],
      "metadata": {
        "id": "2W8y8tJYixLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                      #TRIED USING FLASK\n",
        "\n",
        "\n",
        "\n",
        "#from flask import Flask, request, jsonify\n",
        "# from langchain_chroma import Chroma\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# app = Flask(__name__)\n",
        "\n",
        "# # ... (rest of your imports)\n",
        "\n",
        "# # Load the LLM model and tokenizer (outside the API route)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"mattshumer/Reflection-Llama-3.1-70B\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"mattshumer/Reflection-Llama-3.1-70B\")\n",
        "\n",
        "# # Create the vector store (outside the API route)\n",
        "# vectorstore = Chroma.from_documents(documents=docs)  # Use the default embedding function\n",
        "\n",
        "# @app.route('/query', methods=['POST'])\n",
        "# def query_handler(query):\n",
        "\n",
        "#     results = vectorstore.similarity_search(query)\n",
        "\n",
        "#     ar_vr_docs = [res['content'] for res in results if 'AR/VR' in res['content']]\n",
        "#     simulator_docs = [res['content'] for res in results if 'Simulator' in res['content']]\n",
        "\n",
        "#     # Combine the results into a structured prompt based on the query\n",
        "#     if 'AR/VR' in query:\n",
        "#         context = ' '.join(ar_vr_docs)\n",
        "#     elif 'Simulator' in query:\n",
        "#         context = ' '.join(simulator_docs)\n",
        "#     else:\n",
        "#         context = ' '.join([res['content'] for res in results])  # Default to all results\n",
        "\n",
        "#     # Tokenize the input for the model\n",
        "#     inputs = tokenizer.encode(context + \" \" + query, return_tensors='pt')\n",
        "\n",
        "#     # Generate a response from the model\n",
        "#     outputs = model.generate(inputs, max_length=500, num_return_sequences=1)\n",
        "\n",
        "#     # Decode the response\n",
        "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "#     return response\n"
      ],
      "metadata": {
        "id": "54rsZSN0JMwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETTING UP THE RETRIEVER"
      ],
      "metadata": {
        "id": "yVsIh0u3_Ssh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RETRIEVING THE DATA FROM THE VECTOR DATABASE\n",
        "#Using cosine similarity it will check for similarity between the input query and search through the vector database\n",
        "\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "retrieved_docs = retriever.invoke(\"What kind of services they provide?\")\n"
      ],
      "metadata": {
        "id": "vOwt8hrd_Xgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(retrieved_docs)\n"
      ],
      "metadata": {
        "id": "nbIwbu5J_iJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(retrieved_docs[0].page_content)\n"
      ],
      "metadata": {
        "id": "67n5Uiyw_kpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETUP OPEN AI LLM"
      ],
      "metadata": {
        "id": "YwFUppzh_oWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.4, max_tokens=500)\n",
        "query = st.chat_input(\"Hello!I am ZETRANCE_XR ASSISTANT. How can I help you? \")\n",
        "prompt=query\n",
        "\n",
        "#Setting up the temperature as it sets the score of randomization\n",
        "#1 is completely random output while 0 is not at all random\n",
        "#in max tokens =max characters it gives answer"
      ],
      "metadata": {
        "id": "-3YVxo-r_rwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are the chat-assisstant of ZETRANCE-XR\"\n",
        "    \"If anything other than the information of ZETRANCE-XR is asked\"\n",
        "    \"Say that its a great question but being the chatbot of ZETRANCE XR\"\n",
        "    \"You can only answer questions related to it\"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question. If you don't know the answer, say that you \"\n",
        "    \"don't know. Use three sentences maximum and keep the \"\n",
        "    \"answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "\n",
        "#retrieved document are in context\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "PNsdJc7X_wgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SETTING THE RAG\n",
        "#IT GIVES A MORE COMPREHENSIVE ANSWER TO THE ANSWER GENERATED BY LLM MODELS\n",
        "if query:\n",
        "  question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "  rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "\n",
        "  response = rag_chain.invoke({\"input\": query})\n",
        "  print(response[\"answer\"])\n",
        "\n",
        "  st.write(response[\"answer\"])"
      ],
      "metadata": {
        "id": "1f4r1LdUnnZ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}